# job handling
save= true
out_folder= 'embeddingMLP'
device= 'cuda'
num_jobs= 1 

# dataset
root_folder= 'computed'
data_name= '20240115-105053'
seed_name= 'seedSequence_p10'

# node2vec (arguments passed directly to f.call)
node2vec.embedding_dim= 16
node2vec.walk_length= 5
node2vec.context_size= 1
node2vec.walks_per_node= 3
node2vec.num_negative_samples= 2
node2vec.p= 1.0 # DeepWalk p=q=1
node2vec.q= 1.0
node2vec.sparse= true
# do not add more parameters
embed.batch_size= 32 # related to embedding_dim ?
embed.shuffle= true
embed.num_workers= 10 # how is this related to num_jobs in joblib ?
embed.learning_rate= 2e-2 # optimizer SparseAdam
embed.test_iter= 150 # logreg trained online for testing
embed.max_epochs= 20 # effective, no moving average filter

# model (one-hidden-layer perceptron)
# layer 1
lay1.dropout= 0.1 # 0 deactivated
lay1.activation= true
# layer 2
lay2.hidden_dim= 5 # input dim: embedding_dim*2
lay2.dropout= 0.1 # 0 deactivated
lay2.activation= true

# optimizer Adam or SGD 
opt.name= 'adam' # case insensitive
opt.learning_rate= 1e-2 # Adam
#opt.learning_rate= 1e-2 # SGD
opt.weight_decay= 1e-8 # Adam, SGD
#opt.nesterov= true # SGD
#opt.momentum= 0.1 # SGD
#opt.dampening= 0 # SGD

# training
test_share= 0.3
max_epochs= 500
# early stopping
ma_window= 100
ma_threshold= 1e-3
